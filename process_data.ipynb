{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "#model\n",
    "from sklearn.metrics import r2_score,mean_squared_error,mean_absolute_error\n",
    "\n",
    "import sklearn.linear_model as LM\n",
    "from sklearn import svm\n",
    "from xgboost import XGBRegressor\n",
    "import lightgbm as lgb\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "from pytorch_tabnet.augmentations import RegressionSMOTE\n",
    "from lce import LCERegressor\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils import data\n",
    "\n",
    "import os\n",
    "import re\n",
    "\n",
    "import pickle\n",
    "\n",
    "plt.rcParams['font.sans-serif'] = ['Times New Roman']\n",
    "plt.rc('axes', unicode_minus=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# functions\n",
    "\n",
    "def splitlabel(data,input,output):\n",
    "    X = data[:,input]\n",
    "    y = data[:,output]\n",
    "    return X,y\n",
    "\n",
    "def average_choose_data(data_x,data_y,dividearr,divide_n,aver_num,seed=None):\n",
    "    datanew_x,datanew_y = np.zeros((0,data_x.shape[1])),np.zeros(0)\n",
    "    for ii in range(divide_n):\n",
    "        low,big = dividearr[ii],dividearr[ii+1]\n",
    "        cond = np.where(np.logical_and(data_y>=low,data_y<=big))[0]\n",
    "        tmpx,tmpy = data_x[cond,:],data_y[cond]\n",
    "        if seed != None:\n",
    "            np.random.seed(seed)\n",
    "        verifyidx = np.random.choice(range(tmpy.shape[0]), size=aver_num, replace=False)\n",
    "        datanew_x = np.vstack((datanew_x,tmpx[verifyidx,:]))\n",
    "        datanew_y = np.hstack((datanew_y,tmpy[verifyidx]))\n",
    "\n",
    "    return datanew_x,datanew_y\n",
    "\n",
    "def raw2process(datax): # make sure the features should have the column: *,7v,*,*,*,19v,*,*,37h,37v\n",
    "    datanew = np.zeros((datax.shape[0],6))\n",
    "    datanew[:,0] = datax[:,1] # 7v\n",
    "    datanew[:,1] = datax[:,5] # 19V\n",
    "    datanew[:,2] = datax[:,-1] # 37v\n",
    "    datanew[:,3] = (datax[:,-1]-datax[:,5])/(datax[:,-1]+datax[:,5]) #gr 37v/19v\n",
    "    datanew[:,4] = (datax[:,5]-datax[:,1])/(datax[:,5]+datax[:,1]) #gr 19v/7v\n",
    "    datanew[:,5] = (datax[:,-1]-datax[:,-2])/(datax[:,-1]+datax[:,-2]) #pr37\n",
    "    return datanew\n",
    "\n",
    "def normal(data_xraw,data_yraw):\n",
    "    data_x = np.zeros_like(data_xraw)\n",
    "    minmax = np.zeros((2,data_x.shape[1]+1))\n",
    "    for jj in range(data_x.shape[1]):\n",
    "    # for jj in range(3):\n",
    "        minmax[0,jj],minmax[1,jj] = np.min(data_xraw[:,jj]),np.max(data_xraw[:,jj])\n",
    "        data_x[:,jj] = 2*(data_xraw[:,jj]-minmax[0,jj])/(minmax[1,jj]-minmax[0,jj])-1\n",
    "\n",
    "    minmax[0,-1],minmax[1,-1] = np.min(data_yraw),np.max(data_yraw)\n",
    "    data_y = 2*(data_yraw-minmax[0,-1])/(minmax[1,-1]-minmax[0,-1])-1\n",
    "\n",
    "    return data_x, data_y, minmax\n",
    "\n",
    "def data_MSE(model,X_train,X_test,y_train,y_test):\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    MSEtrain,R2train = mean_squared_error(y_train_pred,y_train),r2_score(y_train_pred,y_train)\n",
    "    MAEtrain = mean_absolute_error(y_train_pred,y_train)\n",
    "    biastrain = np.mean(y_train_pred-y_train)\n",
    "    print('train-MSE ',MSEtrain,' r2 ',R2train,'MAE',MAEtrain,'bias',biastrain)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    MSEtest,R2test = mean_squared_error(y_test_pred,y_test),r2_score(y_test_pred,y_test)\n",
    "    MAEtest = mean_absolute_error(y_test_pred,y_test)\n",
    "    biastest = np.mean(y_test_pred-y_test)\n",
    "    print('test-MSE ',MSEtest,' r2 ',R2test,'MAE',MAEtest,'bias',biastest)\n",
    "    return [np.sqrt(MSEtest),R2test,MAEtest,biastest]\n",
    "\n",
    "def keep_digits(s):\n",
    "    return re.sub(r'\\D', '', s)\n",
    "\n",
    "def get_all_files(folder):\n",
    "    all_files = []\n",
    "    for root, dirs, files in os.walk(folder):\n",
    "        for file in files:\n",
    "            all_files.append(keep_digits(file))\n",
    "    return all_files\n",
    "\n",
    "def normal_out_func(pre,normal_out):\n",
    "    return (pre+1)/2*(normal_out[1]-normal_out[0])+normal_out[0]\n",
    "\n",
    "def get_predict(model,datan,normalize_out):\n",
    "    pre_1 = model.predict(datan)\n",
    "    pre = normal_out_func(pre_1,normalize_out)\n",
    "    pre[np.where(pre<0)[0]]=np.nan\n",
    "    return pre.reshape((-1,1))\n",
    "\n",
    "def process_predict(data_s,normalize_in,normalize_out,models):\n",
    "    data_s2 = 2*(data_s-normalize_in[0,:])/(normalize_in[1,:]-normalize_in[0,:])-1\n",
    "\n",
    "    model_knn,model_et,model_tabnet = models\n",
    "\n",
    "    pre_knn = get_predict(model_knn,data_s2,normalize_out)\n",
    "    pre_et = get_predict(model_et,data_s2,normalize_out)\n",
    "    pre_tabnet = get_predict(model_tabnet,data_s2,normalize_out)\n",
    "\n",
    "    return np.hstack((pre_knn,pre_et,pre_tabnet))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# import train data\n",
    "\n",
    "# pre-process data\n",
    "\n",
    "'''\n",
    "There should be the column like ['lon','lat',...(10 features told in README.md)],\n",
    "or you need to modify the function **splitlabel** and **raw2process**.\n",
    "'''\n",
    "\n",
    "data0 = np.array(pd.read_csv(None))\n",
    "\n",
    "data_x_raw,data_y_raw = splitlabel(data0,range(2,12),1)\n",
    "\n",
    "data_x_p1,data_y_p1 = average_choose_data(data_x_raw,data_y_raw,\n",
    "                                              [ 0.056, 0.123 , 0.1896, 0.2561, 0.3895],\n",
    "                                              4,36,42) # clean data\n",
    "\n",
    "data_x_p1 = raw2process(data_x_p1) # combine the features\n",
    "\n",
    "data_x,data_y,minmax = normal(data_x_p1,data_y_p1) # normalization\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# build all models (13 in total)\n",
    "\n",
    "# X_train,X_test,y_train,y_test = train_test_split(data_x,data_y ,test_size = 0.2)\n",
    "\n",
    "X_train = data_x; y_train = data_y\n",
    "\n",
    "# 0--Multi-Linear Regression\n",
    "model0=LM.LinearRegression()\n",
    "model0.fit(X_train,y_train)\n",
    "\n",
    "# 1--Support Vector Machine\n",
    "C=30;E=0.06;G=0.4\n",
    "model1=svm.SVR(C=C,epsilon=E,gamma=G)\n",
    "model1.fit(X_train,y_train)\n",
    "\n",
    "# 2--XGBoost\n",
    "model2=XGBRegressor()\n",
    "model2.fit(X_train,y_train)\n",
    "\n",
    "# 3(1)--KNN-Average\n",
    "model3=KNeighborsRegressor(n_neighbors=5,weights='uniform')\n",
    "model3.fit(X_train,y_train)\n",
    "\n",
    "# 3(2)--KNN-Euclidean Distance\n",
    "model3_2 = KNeighborsRegressor(n_neighbors=5,weights='distance')\n",
    "model3_2.fit(X_train,y_train)\n",
    "\n",
    "# 4--Gradient Boosting\n",
    "model4 = GradientBoostingRegressor(learning_rate=0.01,n_estimators=100)\n",
    "model4.fit(X_train,y_train)\n",
    "\n",
    "# 5--Random Forest\n",
    "model5 = RandomForestRegressor(n_estimators=100)\n",
    "model5.fit(X_train, y_train)\n",
    "\n",
    "# 6--LightGBM\n",
    "model6 = lgb.LGBMRegressor(metric='rmse')\n",
    "model6.fit(X_train, y_train)\n",
    "\n",
    "# 7--Catboost\n",
    "model7 = CatBoostRegressor(\n",
    "    iterations=1000,  # iteration\n",
    "    learning_rate=0.1,  # lr\n",
    "    depth=7,  # depth\n",
    "    loss_function='RMSE',  # loss function\n",
    "    verbose=100\n",
    ")\n",
    "model7.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# 8--Extra Trees\n",
    "model8 = ExtraTreesRegressor(n_estimators=100)\n",
    "model8.fit(X_train, y_train)\n",
    "\n",
    "# 9--AdaBoost\n",
    "model9 = AdaBoostRegressor(DecisionTreeRegressor(max_depth=4), n_estimators=500)\n",
    "model9.fit(X_train, y_train)\n",
    "\n",
    "# 10--TabNet\n",
    "model10 = TabNetRegressor()\n",
    "model10.fit(X_train, y_train.reshape(-1, 1),\n",
    "           max_epochs=100,\n",
    "        patience=20,\n",
    "        batch_size=12, virtual_batch_size=12,\n",
    "        num_workers=0,\n",
    "        drop_last=False,\n",
    "        augmentations=RegressionSMOTE(p=0.2), #aug\n",
    "           )\n",
    "\n",
    "# 11--LCE\n",
    "model11 = LCERegressor()\n",
    "model11.fit(X_train, y_train)\n",
    "\n",
    "# 12--NN\n",
    "# 13--resnet\n",
    "def np2torch(arr):\n",
    "    arr1 = torch.from_numpy(arr)\n",
    "    arr1 = arr1.float()\n",
    "    return arr1\n",
    "def torch2np(arr):\n",
    "    return arr.detach().numpy()\n",
    "features, labels = np2torch(X_train),np2torch(np.reshape(y_train,(-1,1)))\n",
    "\n",
    "def load_array(data_arrays,batch_size,is_train=True):\n",
    "    dataset = data.TensorDataset(*data_arrays)\n",
    "    return data.DataLoader(dataset,batch_size,shuffle=is_train)\n",
    "\n",
    "batch_size = 12\n",
    "data_iter = load_array((features,labels),batch_size)\n",
    "\n",
    "net1 = nn.Sequential(\n",
    "    nn.Linear(6,64),nn.ReLU(),nn.Dropout(0.25),\n",
    "    nn.Linear(64,128),nn.ReLU(),nn.Dropout(0.25),\n",
    "    nn.Linear(128,64),nn.ReLU(),\n",
    "    nn.Linear(64,1)\n",
    ")\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self,in_features_dim,out_features_dim):\n",
    "        super(ResNet,self).__init__()\n",
    "        self.net2_1 = nn.Sequential(nn.Linear(in_features_dim,64),nn.ReLU(),nn.Dropout(0.25))\n",
    "        self.net2_2 = nn.Sequential(nn.Linear(64,256),nn.ReLU(),nn.Dropout(0.25),\n",
    "                               nn.Linear(256,256),nn.ReLU(),nn.Dropout(0.25),\n",
    "                               nn.Linear(256,64),nn.ReLU())\n",
    "        self.net2_3 = nn.Sequential(nn.Linear(64,out_features_dim))\n",
    "\n",
    "    def forward(self,x):\n",
    "        x1 = self.net2_1(x)\n",
    "        residual = x1\n",
    "        x2 = self.net2_2(x1)\n",
    "        x2 += residual\n",
    "        return self.net2_3(x2)\n",
    "\n",
    "net2 = ResNet(6,1)\n",
    "\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.normal_(m.weight, std = 0.01)\n",
    "net1.apply(init_weights)\n",
    "net2.apply(init_weights)\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "trainer1 = torch.optim.Adam(net1.parameters(),lr=0.0005)\n",
    "trainer2 = torch.optim.Adam([{\n",
    "        \"params\" :net2[0].weight,\n",
    "        'weight_decay':1},{\n",
    "            \"params\":net2[0].bias\n",
    "        }\n",
    "    ],lr = 0.05)\n",
    "\n",
    "num_epochs = 1000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    net1 = net1.train()\n",
    "    for X,y in data_iter:\n",
    "        trainer1.zero_grad()\n",
    "        l = loss(net1(X),y)\n",
    "        l.backward()\n",
    "        trainer1.step()\n",
    "    train_l = loss(net1(features),labels)\n",
    "    if (epoch+1) % 100==0:\n",
    "        print(f'epoch {epoch+1}, loss {train_l:f}')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    net2 = net2.train()\n",
    "    for X,y in data_iter:\n",
    "        trainer2.zero_grad()\n",
    "        l = loss(net2(X),y)\n",
    "        l.backward()\n",
    "        trainer2.step()\n",
    "    train_l = loss(net2(features),labels)\n",
    "    if (epoch+1) % 100==0:\n",
    "        print(f'epoch {epoch+1}, loss {train_l:f}')\n",
    "\n",
    "class Network():\n",
    "    def __init__(self,net):\n",
    "        self.net = net\n",
    "        self.net = self.net.eval()\n",
    "    def predict(self,X):\n",
    "        Xtorch = np2torch(X)\n",
    "        Ytorch = self.net(Xtorch)\n",
    "        return torch2np(Ytorch).reshape((-1))\n",
    "\n",
    "model12 = Network(net1)\n",
    "model13 = Network(net2)\n",
    "\n",
    "models = [model0,model1,model2,model3,model3_2,model4,model5,model6,model7,model8,model9,model10,model11,model12,model13]\n",
    "Names = ['linear','svm','xgb','knn-uni','knn-dis','gb','rf','lgbm','cb','et','adb','tab','lce','nn','res']\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# process the whole data\n",
    "\n",
    "# model_knn = KNeighborsRegressor(n_neighbors=5,weights='uniform')\n",
    "# model_knn.fit(X_train,y_train)\n",
    "#\n",
    "# model_et = ExtraTreesRegressor(n_estimators=100)\n",
    "# model_et.fit(X_train, y_train)\n",
    "#\n",
    "# model_tabnet = TabNetRegressor()\n",
    "# model_tabnet.fit(X_train, y_train.reshape(-1, 1),\n",
    "#            max_epochs=100,\n",
    "#         patience=20,\n",
    "#         batch_size=12, virtual_batch_size=12,\n",
    "#         num_workers=0,\n",
    "#         drop_last=False,\n",
    "#         augmentations=RegressionSMOTE(p=0.2), #aug\n",
    "#            )\n",
    "\n",
    "model_knn = model3\n",
    "model_et = model8\n",
    "model_tabnet = model10\n",
    "\n",
    "with open('knn.pkl', 'wb') as file:\n",
    "    pickle.dump(model_knn, file)\n",
    "with open('et.pkl', 'wb') as file:\n",
    "    pickle.dump(model_et, file)\n",
    "with open('tabnet.pkl', 'wb') as file:\n",
    "    pickle.dump(model_tabnet, file)\n",
    "\n",
    "df = pd.DataFrame(minmax)\n",
    "df.to_csv('minmax.csv')\n",
    "\n",
    "# import outside data\n",
    "\n",
    "directory_urf = 'data/'\n",
    "# with open('knn.pkl', 'rb') as file:\n",
    "#     model_knn = pickle.load(file)\n",
    "# with open('et.pkll', 'rb') as file:\n",
    "#     model_et = pickle.load(file)\n",
    "# with open('tabnet.pkl', 'rb') as file:\n",
    "#     model_tabnet = pickle.load(file)\n",
    "# df = pd.read_csv('minmax.csv')\n",
    "# minmax = np.array(df)\n",
    "\n",
    "filein = get_all_files(directory_urf)\n",
    "normalize_in = minmax[:,:-1]\n",
    "normalize_out = minmax[:,-1]\n",
    "\n",
    "models_final = [model_knn,model_et,model_tabnet]\n",
    "modelfiles = ['KNN','ExtraTrees','TabNet']\n",
    "\n",
    "save_urf = 'result/'\n",
    "\n",
    "for file in filein:\n",
    "    datain = pd.read_csv(directory_urf+file+'.csv')\n",
    "    datain = np.array(datain)\n",
    "    datain_x = datain[:,-10:]\n",
    "    datain_x = raw2process(datain_x)\n",
    "    dataout_y = np.zeros((datain_x.shape[0],3))\n",
    "\n",
    "    tmp_not_nan_idx = []\n",
    "\n",
    "    for tt in range(datain_x.shape[0]):\n",
    "        data_line = datain_x[tt,:]\n",
    "        if np.isnan(data_line).any():\n",
    "            dataout_y[tt,:] = np.nan\n",
    "        else:\n",
    "            tmp_not_nan_idx.append(tt)\n",
    "    print(file+' Start predicting!')\n",
    "    dataout_y[tmp_not_nan_idx,:] = process_predict(datain_x[tmp_not_nan_idx,:],normalize_in,normalize_out,models_final)\n",
    "\n",
    "    dataout = np.hstack((datain[:,:2],dataout_y))\n",
    "\n",
    "    print(file,' ',dataout.shape)\n",
    "    # save as CSV\n",
    "    for tt in range(3):\n",
    "        df = pd.DataFrame(dataout[:,tt+2].reshape((-1,1)))\n",
    "        df.to_csv(save_urf+modelfiles[tt]+'/'+file+'.csv', index=False)\n",
    "    print('------------------------------------------')\n",
    "\n",
    "datain = pd.read_csv(save_urf+filein[0]+'.csv')\n",
    "datain = np.array(datain)\n",
    "df = pd.DataFrame(datain[:,:2])\n",
    "df.to_csv(save_urf+'lonlat.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}