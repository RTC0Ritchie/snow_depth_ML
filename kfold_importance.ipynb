{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "#model\n",
    "from sklearn.metrics import r2_score,mean_squared_error,mean_absolute_error\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "from sklearn import svm\n",
    "from xgboost import XGBRegressor\n",
    "import lightgbm as lgb\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "from pytorch_tabnet.augmentations import RegressionSMOTE\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from lce import LCERegressor\n",
    "\n",
    "plt.rcParams['font.sans-serif'] = ['Times New Roman']\n",
    "plt.rc('axes', unicode_minus=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def splitlabel(data,input,output):\n",
    "    X = data[:,input]\n",
    "    y = data[:,output]\n",
    "    return X,y\n",
    "\n",
    "def average_choose_data(data_x,data_y,dividearr,divide_n,aver_num,seed=None):\n",
    "    datanew_x,datanew_y = np.zeros((0,data_x.shape[1])),np.zeros(0)\n",
    "    for ii in range(divide_n):\n",
    "        low,big = dividearr[ii],dividearr[ii+1]\n",
    "        cond = np.where(np.logical_and(data_y>=low,data_y<=big))[0]\n",
    "        tmpx,tmpy = data_x[cond,:],data_y[cond]\n",
    "        if seed != None:\n",
    "            np.random.seed(seed)\n",
    "        verifyidx = np.random.choice(range(tmpy.shape[0]), size=aver_num, replace=False)\n",
    "        datanew_x = np.vstack((datanew_x,tmpx[verifyidx,:]))\n",
    "        datanew_y = np.hstack((datanew_y,tmpy[verifyidx]))\n",
    "\n",
    "    return datanew_x,datanew_y\n",
    "\n",
    "def raw2process(datax):\n",
    "    datanew = np.zeros((datax.shape[0],6))\n",
    "    datanew[:,0] = datax[:,1] # 7v\n",
    "    datanew[:,1] = datax[:,5] # 19V\n",
    "    datanew[:,2] = datax[:,-1] # 37v\n",
    "    datanew[:,3] = (datax[:,-1]-datax[:,5])/(datax[:,-1]+datax[:,5]) #gr 37v/19v\n",
    "    datanew[:,4] = (datax[:,5]-datax[:,1])/(datax[:,5]+datax[:,1]) #gr 19v/7v\n",
    "    datanew[:,5] = (datax[:,-1]-datax[:,-2])/(datax[:,-1]+datax[:,-2]) #pr37\n",
    "    return datanew\n",
    "\n",
    "def normal(data_xraw,data_yraw):\n",
    "    data_x = np.zeros_like(data_xraw)\n",
    "    minmax = np.zeros((2,data_x.shape[1]+1))\n",
    "    for jj in range(data_x.shape[1]):\n",
    "        minmax[0,jj],minmax[1,jj] = np.min(data_xraw[:,jj]),np.max(data_xraw[:,jj])\n",
    "        data_x[:,jj] = 2*(data_xraw[:,jj]-minmax[0,jj])/(minmax[1,jj]-minmax[0,jj])-1\n",
    "\n",
    "    minmax[0,-1],minmax[1,-1] = np.min(data_yraw),np.max(data_yraw)\n",
    "    data_y = 2*(data_yraw-minmax[0,-1])/(minmax[1,-1]-minmax[0,-1])-1\n",
    "\n",
    "    return data_x, data_y, minmax\n",
    "\n",
    "def normal_out_func(pre,normal_out):\n",
    "    return (pre+1)/2*(normal_out[1]-normal_out[0])+normal_out[0]\n",
    "def get_predict(model,datan,normal_out):\n",
    "    pre_1 = model.predict(datan)\n",
    "    pre = normal_out_func(pre_1,normal_out)\n",
    "    return pre\n",
    "def process_predict(data_s,normal_in,normal_out,model):\n",
    "    data_s2 = 2*(data_s-normal_in[0,:])/(normal_in[1,:]-normal_in[0,:])-1\n",
    "    pre3_1 = get_predict(model,data_s2,normal_out)\n",
    "    return  pre3_1\n",
    "\n",
    "class normal_model:\n",
    "    def __init__(self,model,istabnet=False):\n",
    "        self.model = model\n",
    "        self.istabnet = istabnet\n",
    "\n",
    "    def fit(self,data_xraw,data_yraw):\n",
    "        self.X_train,self.y_train,self.minmax = None, None, None\n",
    "        self.X_train,self.y_train,self.minmax = normal(data_xraw,data_yraw)\n",
    "\n",
    "        if self.istabnet:\n",
    "            self.model.fit(self.X_train,self.y_train.reshape(-1, 1),\n",
    "                   max_epochs=100,\n",
    "                    patience=20,\n",
    "                    batch_size=24, virtual_batch_size=12,\n",
    "                    num_workers=0,\n",
    "                    drop_last=False,\n",
    "                    augmentations=RegressionSMOTE(p=0.2), #aug\n",
    "                   )\n",
    "\n",
    "        else:\n",
    "            self.model.fit(self.X_train,self.y_train)\n",
    "\n",
    "    def predict(self,X_test):\n",
    "        self.X_test = X_test\n",
    "        normalize_in = self.minmax[:,:-1]\n",
    "        normalize_out = self.minmax[:,-1]\n",
    "        self.y_pre = process_predict(self.X_test,normalize_in,normalize_out,self.model)\n",
    "        if self.istabnet:\n",
    "            self.y_pre = self.y_pre.reshape(-1)\n",
    "        return self.y_pre\n",
    "\n",
    "    def myscore(self,X_test,y_test,isshow=False):\n",
    "        y_test_pred = self.predict(X_test)\n",
    "        MSEtest,R2test = mean_squared_error(y_test,y_test_pred),r2_score(y_test,y_test_pred)\n",
    "        MAEtest = mean_absolute_error(y_test,y_test_pred)\n",
    "        biastest = np.mean(y_test_pred-y_test)\n",
    "        if isshow:\n",
    "            print('test-RMSE ',np.sqrt(MSEtest),' r2 ',R2test,'MAE',MAEtest,'bias',biastest)\n",
    "        return np.array([np.sqrt(MSEtest),R2test,MAEtest,biastest])\n",
    "\n",
    "    def importance(self,data_xraw,data_yraw):\n",
    "        self.X_train,self.y_train,self.minmax = None, None, None\n",
    "        self.X_train,self.y_train,self.minmax = normal(data_xraw,data_yraw)\n",
    "        if self.istabnet:\n",
    "            self.y_train = self.y_train.reshape(-1, 1)\n",
    "\n",
    "        if self.istabnet:\n",
    "            self.model.fit(self.X_train,self.y_train.reshape(-1, 1),\n",
    "                   max_epochs=100,\n",
    "                    patience=20,\n",
    "                    batch_size=24, virtual_batch_size=12,\n",
    "                    num_workers=0,\n",
    "                    drop_last=False,\n",
    "                    augmentations=RegressionSMOTE(p=0.2), #aug\n",
    "                   )\n",
    "\n",
    "        else:\n",
    "            self.model.fit(self.X_train,self.y_train)\n",
    "        result = permutation_importance(self.model, self.X_train, self.y_train, n_repeats=5,scoring='neg_mean_squared_error')\n",
    "\n",
    "        importances = result.importances_mean\n",
    "        return importances\n",
    "\n",
    "def split_data_into_folds(data, labels, n_splits=5, shuffle=True, random_state=None):\n",
    "    kf = KFold(n_splits=n_splits, shuffle=shuffle, random_state=random_state)\n",
    "    folds = []\n",
    "    for train_index, test_index in kf.split(data):\n",
    "        train_data, test_data = data[train_index], data[test_index]\n",
    "        train_labels, test_labels = labels[train_index], labels[test_index]\n",
    "        folds.append((train_data, train_labels, test_data, test_labels))\n",
    "    return folds"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data0 = np.array(pd.read_csv(None))\n",
    "\n",
    "data_x11,data_yraw1 = splitlabel(data0,range(2,12),1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''\n",
    "K-fold\n",
    "\n",
    "You need to create a folder named 'K_fold'\n",
    "'''\n",
    "\n",
    "num_epoch = 100\n",
    "num_fold = 5\n",
    "colomn = ['knn-u','et','tab','gb','knn-d','svm','xgb','rf','lgb','cb','adb','lce','mlp']\n",
    "\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "\n",
    "    data_x1,data_yraw = average_choose_data(data_x11,data_yraw1,\n",
    "                                              [ 0.056, 0.123 , 0.1896, 0.2561, 0.3895],\n",
    "                                              4,36,np.random.random_integers(0,100000,1)[0]) # you can change the seed to get fixed result\n",
    "    data_xraw = raw2process(data_x1)\n",
    "\n",
    "\n",
    "    folds = split_data_into_folds(data_xraw, data_yraw, n_splits=num_fold, shuffle=True, random_state=np.random.random_integers(0,100000,1)[0])\n",
    "\n",
    "    result = np.zeros((13,num_fold,4))\n",
    "\n",
    "    for i, (train_data, train_labels, test_data, test_labels) in enumerate(folds):\n",
    "        model1 = normal_model(KNeighborsRegressor(weights='uniform'))\n",
    "        model2 = normal_model(ExtraTreesRegressor(n_estimators=100))\n",
    "        model3 = normal_model(TabNetRegressor(verbose=0),True)\n",
    "\n",
    "        model4 = normal_model(GradientBoostingRegressor(learning_rate=0.01,n_estimators=100))\n",
    "        model5 = normal_model(KNeighborsRegressor(weights='distance'))\n",
    "        model6 = normal_model(svm.SVR(C=30,epsilon=0.06,gamma=0.4))\n",
    "        model7 = normal_model(XGBRegressor())\n",
    "        model8 = normal_model(RandomForestRegressor(n_estimators=100))\n",
    "        model9 = normal_model(lgb.LGBMRegressor(metric='rmse'))\n",
    "        model10 = normal_model(CatBoostRegressor(\n",
    "            iterations=1000,\n",
    "            learning_rate=0.1,\n",
    "            depth=7,\n",
    "            loss_function='RMSE',\n",
    "            verbose=100\n",
    "        ))\n",
    "        model11 = normal_model(AdaBoostRegressor(DecisionTreeRegressor(max_depth=4), n_estimators=500))\n",
    "        model12 = normal_model(LCERegressor())\n",
    "        model13 = normal_model(MLPRegressor(solver='adam', alpha=1e-5,hidden_layer_sizes=(64, 64), random_state=np.random.random_integers(0,100000,1)[0]))\n",
    "\n",
    "\n",
    "        model1.fit(train_data, train_labels)\n",
    "        model2.fit(train_data, train_labels)\n",
    "        model3.fit(train_data, train_labels)\n",
    "        model4.fit(train_data, train_labels)\n",
    "        model5.fit(train_data, train_labels)\n",
    "        model6.fit(train_data, train_labels)\n",
    "        model7.fit(train_data, train_labels)\n",
    "        model8.fit(train_data, train_labels)\n",
    "        model9.fit(train_data, train_labels)\n",
    "        model10.fit(train_data, train_labels)\n",
    "        model11.fit(train_data, train_labels)\n",
    "        model12.fit(train_data, train_labels)\n",
    "        model13.fit(train_data, train_labels)\n",
    "\n",
    "\n",
    "        result[0,i,:] = model1.myscore(test_data, test_labels)\n",
    "        result[1,i,:] = model2.myscore(test_data, test_labels)\n",
    "        result[2,i,:] = model3.myscore(test_data, test_labels)\n",
    "        result[3,i,:] = model4.myscore(test_data, test_labels)\n",
    "        result[4,i,:] = model5.myscore(test_data, test_labels)\n",
    "        result[5,i,:] = model6.myscore(test_data, test_labels)\n",
    "        result[6,i,:] = model7.myscore(test_data, test_labels)\n",
    "        result[7,i,:] = model8.myscore(test_data, test_labels)\n",
    "        result[8,i,:] = model9.myscore(test_data, test_labels)\n",
    "        result[9,i,:] = model10.myscore(test_data, test_labels)\n",
    "        result[10,i,:] = model11.myscore(test_data, test_labels)\n",
    "        result[11,i,:] = model12.myscore(test_data, test_labels)\n",
    "        result[12,i,:] = model13.myscore(test_data, test_labels)\n",
    "\n",
    "    result = np.mean(result,axis=1)\n",
    "    result = result.T\n",
    "\n",
    "    df = pd.DataFrame(result,columns=colomn)\n",
    "    df.to_csv('K_fold\\\\'+str(epoch+1)+'.csv',index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Compute the statistical features\n",
    "\n",
    "statis = np.zeros((4,13,num_epoch))\n",
    "for epoch in range(num_epoch):\n",
    "    statis[:,:,epoch] = np.array(pd.read_csv('K_fold\\\\'+str(epoch+1)+'.csv'))\n",
    "\n",
    "statis_mean = np.nanmean(statis,axis=2)\n",
    "statis_std = np.nanstd(statis,axis=2)\n",
    "\n",
    "df = pd.DataFrame(statis_mean,columns=colomn)\n",
    "df.to_csv('statis_mean.csv',index=False)\n",
    "df = pd.DataFrame(statis_std,columns=colomn)\n",
    "df.to_csv('statis_std.csv',index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''\n",
    "Importance analysis\n",
    "\n",
    "You need to create a folder named 'importance'\n",
    "'''\n",
    "\n",
    "num_epoch = 100\n",
    "importances = np.zeros((num_epoch,6,3))\n",
    "column = ['knn','et','tab']\n",
    "for epoch in range(num_epoch):\n",
    "    data_x1,data_yraw = average_choose_data(data_x11,data_yraw1,\n",
    "                                              [ 0.056, 0.123 , 0.1896, 0.2561, 0.3895],\n",
    "                                              4,36,np.random.random_integers(0,100000,1)[0])\n",
    "    data_xraw = raw2process(data_x1)\n",
    "\n",
    "    model1 = normal_model(KNeighborsRegressor(weights='uniform'))\n",
    "    model2 = normal_model(ExtraTreesRegressor(n_estimators=100))\n",
    "    model3 = normal_model(TabNetRegressor(verbose=0),True)\n",
    "\n",
    "    importances[epoch,:,0] = model1.importance(data_xraw,data_yraw)\n",
    "    importances[epoch,:,1] = model2.importance(data_xraw,data_yraw)\n",
    "    importances[epoch,:,2] = model3.importance(data_xraw,data_yraw)\n",
    "\n",
    "    df = pd.DataFrame(importances[epoch,:,:],columns=column)\n",
    "    df.to_csv('importance\\\\'+str(epoch+1)+'.csv',index=False)\n",
    "\n",
    "importances = np.mean(importances,axis=0)\n",
    "# print(importances)\n",
    "df = pd.DataFrame(importances,columns=column)\n",
    "df.to_csv('importance.csv',index=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}